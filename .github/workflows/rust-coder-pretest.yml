name: Rust Coder Pretest

on:
  workflow_dispatch:
    inputs:
      repo_url:
        description: "GitHub repository URL to port to Rust"
        required: true
        type: string
        default: "https://github.com/mufeedvh/code2prompt"
      language:
        description: "Source language"
        required: false
        type: choice
        options:
          - python
          - cpp
        default: "python"
      llm_provider:
        description: "LLM provider"
        required: false
        type: choice
        options:
          - openai
          - claude
          - gemini
        default: "openai"
      model:
        description: "Model name"
        required: false
        type: string
        default: "gpt-4o-mini"

jobs:
  port-to-rust:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Setup Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true
          
      - name: Install dependencies
        run: |
          pip install code2prompt requests
          
      - name: Clone source repository
        run: |
          echo "Cloning repository: ${{ inputs.repo_url }}"
          rm -rf source_repo
          git clone "${{ inputs.repo_url }}" source_repo
          echo "Repository cloned successfully"
          echo "Files found:"
          find source_repo -type f -name "*.py" -o -name "*.cpp" -o -name "*.h" | head -10
          
      - name: Generate code prompt
        id: prompt
        run: |
          echo "Generating code prompt..."
          LANGUAGE="${{ inputs.language }}"
          
          # Simple approach: find and concatenate source files
          if [ "$LANGUAGE" = "python" ]; then
            find source_repo -name "*.py" -type f -exec cat {} \; > code_prompt.txt
          else
            find source_repo -name "*.cpp" -o -name "*.h" -o -name "*.hpp" -o -name "*.cc" -o -name "*.cxx" -type f -exec cat {} \; > code_prompt.txt
          fi
          
          PROMPT_SIZE=$(wc -c < code_prompt.txt)
          echo "Prompt generated: $PROMPT_SIZE characters"
          echo "prompt_size=$PROMPT_SIZE" >> $GITHUB_OUTPUT
          
      - name: Create Rust porting prompt
        run: |
          LANGUAGE="${{ inputs.language }}"
          PROMPT_CONTENT=$(cat code_prompt.txt)
          
          cat > rust_prompt.txt << EOF
          You are an expert Rust developer. Port this $LANGUAGE project to Rust.
          
          SOURCE CODE:
          $PROMPT_CONTENT
          
          INSTRUCTIONS:
          1. Create a single Rust file (src/main.rs) that implements the core functionality
          2. Use only the Rust standard library - no external crates
          3. Make it compile and run successfully
          4. Output ONLY the Rust code inside a \`\`\`rust code block
          5. No explanations, comments, or markdown outside the code block
          
          Output the complete Rust code:
          EOF
          
      - name: Call LLM API
        id: llm
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          LLM_PROVIDER="${{ inputs.llm_provider }}"
          MODEL="${{ inputs.model }}"
          
          # Read and escape the prompt content for JSON
          PROMPT_CONTENT=$(cat rust_prompt.txt | jq -Rs .)
          
          if [ "$LLM_PROVIDER" = "openai" ]; then
            echo "Checking OpenAI API key..."
            if [ -z "$OPENAI_API_KEY" ]; then
              echo "‚ùå OPENAI_API_KEY not set"
              echo "This is expected when running locally without secrets configured."
              echo "In production, make sure to set OPENAI_API_KEY as a GitHub secret."
              exit 1
            fi
            echo "‚úÖ OpenAI API key is set (length: ${#OPENAI_API_KEY} characters)"
            
            echo "Calling OpenAI API..."
            
            # Create JSON payload with proper escaping
            jq -n \
              --arg model "$MODEL" \
              --arg prompt "$(cat rust_prompt.txt)" \
              '{
                "model": $model,
                "messages": [
                  {"role": "system", "content": "You are an expert Rust developer. Output only Rust code in code blocks."},
                  {"role": "user", "content": $prompt}
                ],
                "max_tokens": 4000,
                "temperature": 0.1
              }' > request_payload.json
            
            curl -s -X POST "https://api.openai.com/v1/chat/completions" \
              -H "Authorization: Bearer $OPENAI_API_KEY" \
              -H "Content-Type: application/json" \
              -d @request_payload.json > llm_response.json
            
            # Check for errors
            if jq -e '.error' llm_response.json > /dev/null 2>&1; then
              echo "‚ùå OpenAI API Error:"
              jq -r '.error.message' llm_response.json
              exit 1
            fi
            
            jq -r '.choices[0].message.content' llm_response.json > llm_output.txt
            
          elif [ "$LLM_PROVIDER" = "claude" ]; then
            echo "Checking Claude API key..."
            if [ -z "$ANTHROPIC_API_KEY" ]; then
              echo "‚ùå ANTHROPIC_API_KEY not set"
              echo "This is expected when running locally without secrets configured."
              echo "In production, make sure to set ANTHROPIC_API_KEY as a GitHub secret."
              exit 1
            fi
            echo "‚úÖ Claude API key is set (length: ${#ANTHROPIC_API_KEY} characters)"
            
            echo "Calling Claude API..."
            
            # Create JSON payload with proper escaping
            jq -n \
              --arg model "$MODEL" \
              --arg prompt "$(cat rust_prompt.txt)" \
              '{
                "model": $model,
                "max_tokens": 4000,
                "messages": [
                  {"role": "user", "content": $prompt}
                ]
              }' > request_payload.json
            
            curl -s -X POST "https://api.anthropic.com/v1/messages" \
              -H "x-api-key: $ANTHROPIC_API_KEY" \
              -H "Content-Type: application/json" \
              -H "anthropic-version: 2023-06-01" \
              -d @request_payload.json > llm_response.json
            
            # Check for errors
            if jq -e '.error' llm_response.json > /dev/null 2>&1; then
              echo "‚ùå Claude API Error:"
              jq -r '.error.message' llm_response.json
              exit 1
            fi
            
            jq -r '.content[0].text' llm_response.json > llm_output.txt
            
          elif [ "$LLM_PROVIDER" = "gemini" ]; then
            echo "Checking Gemini API key..."
            if [ -z "$GEMINI_API_KEY" ]; then
              echo "‚ùå GEMINI_API_KEY not set"
              echo "This is expected when running locally without secrets configured."
              echo "In production, make sure to set GEMINI_API_KEY as a GitHub secret."
              exit 1
            fi
            echo "‚úÖ Gemini API key is set (length: ${#GEMINI_API_KEY} characters)"
            
            echo "Calling Gemini API..."
            
            # Create JSON payload with proper escaping
            jq -n \
              --arg model "$MODEL" \
              --arg prompt "$(cat rust_prompt.txt)" \
              '{
                "contents": [
                  {
                    "parts": [
                      {"text": $prompt}
                    ]
                  }
                ],
                "generationConfig": {
                  "maxOutputTokens": 4000,
                  "temperature": 0.1
                }
              }' > request_payload.json
            
            curl -s -X POST "https://generativelanguage.googleapis.com/v1beta/models/${MODEL}:generateContent?key=${GEMINI_API_KEY}" \
              -H "Content-Type: application/json" \
              -d @request_payload.json > llm_response.json
            
            # Check for errors
            if jq -e '.error' llm_response.json > /dev/null 2>&1; then
              echo "‚ùå Gemini API Error:"
              jq -r '.error.message' llm_response.json
              exit 1
            fi
            
            jq -r '.candidates[0].content.parts[0].text' llm_response.json > llm_output.txt
          else
            echo "‚ùå Unknown LLM provider: $LLM_PROVIDER"
            exit 1
          fi
          
          echo "LLM response received: $(wc -c < llm_output.txt) characters"
          
      - name: Extract Rust code
        run: |
          echo "Extracting Rust code from LLM response..."
          python3 scripts/extract_rust.py < llm_output.txt > main.rs
          echo "Rust code extracted: $(wc -c < main.rs) characters"
          
          # Check if we got any meaningful Rust code
          if [ ! -s main.rs ]; then
            echo "‚ö†Ô∏è  No Rust code extracted, creating a basic template..."
            cat > main.rs << 'EOF'
fn main() {
    println!("Rust port of the original project");
    println!("This is a placeholder - the LLM did not generate valid Rust code");
}
EOF
          fi
          
      - name: Create Rust project and validate
        id: compile
        run: |
          echo "Creating Rust project..."
          cargo new rust-port --bin
          cp main.rs rust-port/src/main.rs
          
          echo "Validating Rust code structure..."
          cd rust-port
          
          # Check if the file contains valid Rust syntax patterns
          if grep -q "fn main" src/main.rs; then
            echo "‚úÖ Rust code contains main function"
            HAS_MAIN=true
          else
            echo "‚ö†Ô∏è  Warning: No main function found in Rust code"
            HAS_MAIN=false
          fi
          
          # Check for basic Rust syntax
          if grep -q "use " src/main.rs || grep -q "fn " src/main.rs || grep -q "let " src/main.rs; then
            echo "‚úÖ Rust code contains valid syntax patterns"
            HAS_SYNTAX=true
          else
            echo "‚ö†Ô∏è  Warning: No clear Rust syntax patterns found"
            HAS_SYNTAX=false
          fi
          
          # Try to compile but don't fail the workflow
          echo "Attempting to compile Rust project..."
          if cargo check --quiet 2>/dev/null; then
            echo "‚úÖ Rust project compiles successfully!"
            echo "compilation_status=success" >> $GITHUB_OUTPUT
          else
            echo "‚ö†Ô∏è  Compilation check failed (this is expected for LLM-generated code)"
            echo "compilation_status=check_failed" >> $GITHUB_OUTPUT
            
            # Show compilation errors for debugging
            echo "Compilation errors:"
            cargo check 2>&1 | head -20 || true
          fi
          
          # Always continue regardless of compilation result
          echo "Rust project created and validated"
          
      - name: Create demo script
        run: |
          cat > demo_script.sh << 'EOF'
          #!/bin/bash
          echo "=== Rust Coder Pretest Demo ==="
          echo "Source Repository: ${{ inputs.repo_url }}"
          echo "Language: ${{ inputs.language }}"
          echo "LLM Provider: ${{ inputs.llm_provider }}"
          echo "Model: ${{ inputs.model }}"
          echo ""
          echo "1. Repository cloned and analyzed"
          echo "2. Code prompt generated: ${{ steps.prompt.outputs.prompt_size }} characters"
          echo "3. LLM generated Rust code"
          echo "4. Rust project created and validated: ${{ steps.compile.outputs.compilation_status }}"
          echo ""
          echo "=== Demo Complete ==="
          EOF
          
          chmod +x demo_script.sh
          
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: rust-coder-results
          path: |
            source_repo/
            rust-port/
            code_prompt.txt
            llm_output.txt
            main.rs
            demo_script.sh
            llm_response.json
          retention-days: 30
          
      - name: Create summary
        run: |
          echo "## Rust Coder Pretest Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìä Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Source Repository**: ${{ inputs.repo_url }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Language**: ${{ inputs.language }}" >> $GITHUB_STEP_SUMMARY
          echo "- **LLM Provider**: ${{ inputs.llm_provider }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Model**: ${{ inputs.model }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Prompt Size**: ${{ steps.prompt.outputs.prompt_size }} characters" >> $GITHUB_STEP_SUMMARY
          echo "- **Compilation**: ${{ steps.compile.outputs.compilation_status }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìÅ Generated Files" >> $GITHUB_STEP_SUMMARY
          echo "All files are available in the artifacts for download." >> $GITHUB_STEP_SUMMARY
